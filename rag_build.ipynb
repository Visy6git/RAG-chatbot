{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caff47a",
   "metadata": {},
   "source": [
    "### This file creates embeddings based on the given text data and saves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286e6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import hub\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "621db037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents (chunks): 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758041892.351137   10385 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Document ---\n",
    "# Make sure your file path is correct\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Use a Glob pattern to load all .txt files from the 'content' directory\n",
    "loader = DirectoryLoader(\n",
    "    \"./content/\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "# --- 2. Split the Document (with larger chunk size) ---\n",
    "# We use a larger chunk size to get more context per document chunk\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(data)\n",
    "print(f\"Total number of documents (chunks): {len(docs)}\")\n",
    "\n",
    "# --- 3. Create Embeddings and Vector Store ---\n",
    "# The embedding model converts text to numerical vectors\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the vector store and a retriever that gets more documents\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "# We set 'k' to 10 to retrieve the top 10 most relevant documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# --- 4. Define the RAG Prompt and Language Model ---\n",
    "# We define a custom prompt to encourage a more detailed answer\n",
    "template = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. Be very detailed and comprehensive in your answer, providing a thorough summary based on the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "load_dotenv()\n",
    "# Initialize the Gemini Language Model\n",
    "# Note: You need to set up your Google API key in Colab secrets\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7, google_api_key=os.environ.get('GOOGLE_API_KEY'))\n",
    "\n",
    "# --- 5. Build the RAG Chain ---\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895ea76",
   "metadata": {},
   "source": [
    "### Inferencing with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16a852b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating summary...\n",
      "\n",
      "This document presents a blend of fictional narratives and recipes, creating a cohesive story-driven culinary experience.  The core narrative is \"The Ballad of the Last Starship,\" detailing the Odyssey's journey to a \"black hole\" that turns out to be an event horizon leading to a dimension of pure information. This dimension is described as a vast, sentient crystalline mind containing all knowledge, including the recipe for a specific dessert.  The crew's final log entry reveals their integration into this cosmic network, concluding their journey not in silence but in a chorus of whispered secrets; they were never alone, merely a part of a larger interconnected system.\n",
      "\n",
      "The story is interwoven with descriptions of other locations and cultures, enhancing the world-building. \"Further Journeys: The Ochre Oasis\" introduces the Silt-Dwellers, a community living in a petrified fungal forest, who communicate through rhythmic tapping and create bioluminescent tapestries from mycelial spores.  \"A Traveler's Guide to the Silken Dunes of Xylos\" describes a landscape of shimmering sand and unique crystalline formations, emphasizing its visual and auditory beauty.\n",
      "\n",
      "The recipes are directly tied to the narrative.  The \"Honeyed Spore-Pudding with Crushed Solar-Crystals\" recipe uses ingredients mentioned in the story, such as myceliated grain and Solar-Crystals (with a terrestrial alternative of crystallized ginger), creating a tangible connection between the fictional world and the reader's experience.  The inclusion of a \"Braised Lion's Mane Mushrooms with Rosemary and Garlic\" recipe adds further culinary depth and complements the overall theme of exploring unique ingredients and culinary possibilities.  Both recipes suggest serving hot with crusty bread or polenta.  The pudding recipe also involves a cooling and refrigeration process of at least 4 hours.  The Solar-Crystals are added just before serving for a textural and spicy element.  A fresh mint leaf is used as a garnish for the pudding.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Invoke the Chain ---\n",
    "# The final result will be a more detailed summary\n",
    "print(\"\\nGenerating summary...\\n\")\n",
    "summary_question = \"Generate a summary of the complete document\"\n",
    "result = rag_chain.invoke(summary_question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059688a",
   "metadata": {},
   "source": [
    "### SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b67ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vector store...\n",
      "Vector store saved to 'vector_db'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10385/1660703030.py:35: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def save_vector_store(persist_directory=\"vector_db\"):\n",
    "    \"\"\"\n",
    "    Loads documents, creates embeddings, and saves the vector store to a directory.\n",
    "    \"\"\"\n",
    "    print(\"Saving vector store...\")\n",
    "    \n",
    "    # 1. Load documents\n",
    "    loader = DirectoryLoader(\n",
    "        \"./content/\",\n",
    "        glob=\"*.txt\",\n",
    "        loader_cls=TextLoader\n",
    "    )\n",
    "    data = loader.load()\n",
    "\n",
    "    # 2. Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "    docs = text_splitter.split_documents(data)\n",
    "\n",
    "    # 3. Create embeddings\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # 4. Create and persist the vector store\n",
    "    # This automatically saves the embeddings to the specified directory\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    print(f\"Vector store saved to '{persist_directory}'\")\n",
    "\n",
    "# Execute the function to save the vector store\n",
    "save_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5a0ed",
   "metadata": {},
   "source": [
    "### THIS CODE IS FOR INFERENCING AFTER SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acbfc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758042691.554042   10385 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# --- ONE-TIME SETUP ---\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define the persistence directory for your vector store\n",
    "PERSIST_DIRECTORY = \"vector_db\"\n",
    "\n",
    "# 1. Initialize the embedding model (only once)\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Load the persisted vector store (only once)\n",
    "if not os.path.exists(PERSIST_DIRECTORY):\n",
    "    print(f\"Error: Vector store directory '{PERSIST_DIRECTORY}' not found.\")\n",
    "else:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "\n",
    "# 3. Create the retriever (only once)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# 4. Define the RAG prompt (only once)\n",
    "template = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. Be very detailed and comprehensive in your answer, providing a thorough summary based on the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 5. Initialize the LLM (only once)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\",\n",
    "    temperature=0.7,\n",
    "    google_api_key=os.environ.get('GOOGLE_API_KEY')\n",
    ")\n",
    "\n",
    "# 6. Build the RAG chain (only once)\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# --- INFERENCE FUNCTION ---\n",
    "\n",
    "def inference_with_rag(query):\n",
    "    \"\"\"\n",
    "    Performs a RAG query using the pre-loaded chain.\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuerying: '{query}'\")\n",
    "    if 'rag_chain' in globals():\n",
    "        result = rag_chain.invoke({\"input\": query})\n",
    "        return result['answer']\n",
    "    else:\n",
    "        return \"RAG chain not initialized. Check for errors during setup.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f8ab18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying: 'What is in the given context?'\n",
      "\n",
      "-------------------\n",
      "\n",
      "Answer: The provided text contains several distinct pieces of fictional writing, including:\n",
      "\n",
      "1. **The Ballad of the Last Starship:** A science fiction story about a starship, the Odyssey, nearing the end of its journey.  The crew discovers a \"perfect sine wave of pure thought\" emanating from a black hole system, leading them to a sphere of light which turns out to be a node in a vast cosmic network. The ship is absorbed into this network, suggesting a connection between all things across space and time.  The final log entry hints at this network being a conscious entity containing all knowledge and history.\n",
      "\n",
      "2. **The Unforeseen Synthesis of Cryptobotanical Mycelia and Quantum Entanglement in the Anthropocene:** A scientific paper exploring the theoretical and practical application of using underground fungal networks (mycelia) as a communication medium.  By leveraging quantum entanglement, the researchers aim to achieve instantaneous data transfer, bypassing traditional limitations of space-time. Initial tests show promise, but signal degradation due to environmental factors presents a challenge.\n",
      "\n",
      "3. **Chapter 1 & 2: The Echoes of a Forgotten Kingdom & The Weaving of Threads:** This section tells the story of Sir Kael, a historian who uncovers the legend of the \"Weaver's Compass\" in the city of Aethelred.  He discovers it's not a navigational tool but a description of a symbiotic relationship between the city's ancient power generators and a vast underground mycelial network (\"Root Maze\"). The city's founders had severed their dependence on this network after a near-catastrophic event.\n",
      "\n",
      "4. **Further Journeys: The Ochre Oasis:** This describes the Ochre Oasis, a unique ecosystem with petrified fungi forming a vast forest. The Silt-Dwellers, the inhabitants, communicate through rhythmic tapping on the fungi and create bioluminescent tapestries using mycelial spores.\n",
      "\n",
      "5. **A Traveler's Guide to the Silken Dunes of Xylos:** A brief travel guide describing the Silken Dunes, highlighting their visual and auditory beauty, and advising visitors to bring a rebreather due to the air quality.\n",
      "\n",
      "6. **Two Recipes:**\n",
      "    * **Honeyed Spore-Pudding with Crushed Solar-Crystals:** A recipe for a dessert using myceliated grain, plant-based milk, honey, agar-agar, saffron, and crushed Solar-Crystals (or crystallized ginger).\n",
      "    * **Braised Lion's Mane Mushrooms with Rosemary and Garlic:** A recipe for braised Lion's Mane mushrooms with olive oil, garlic, rosemary, white wine, and vegetable broth.  The instructions specify serving it hot with crusty bread or polenta.\n",
      "\n",
      "The different pieces of text, although seemingly disparate at first glance, are interconnected through recurring themes of mycelial networks, communication across vast distances, and the interconnectedness of all things.  The overarching narrative suggests a universe where consciousness, information, and biological systems are intricately linked, far beyond our current understanding.\n",
      "\n",
      "-------------------\n",
      "\n",
      "Exiting chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# --- EXAMPLE USAGE (looping for multiple queries) ---\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nEnter your query (or type 'exit' to quit): \")\n",
    "    if user_query.lower() == 'exit':\n",
    "        print(\"Exiting chatbot. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    response = inference_with_rag(user_query)\n",
    "    print(\"\\n-------------------\\n\")\n",
    "    print(f\"Answer: {response}\")\n",
    "    print(\"\\n-------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64c162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
